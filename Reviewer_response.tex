\documentclass[letterpaper,12pt]{report}
\usepackage{fixltx2e}
\usepackage{amssymb,amsfonts,amsmath}
\usepackage{color,graphicx}
\usepackage{siunitx}
\usepackage{fixltx2e}
\newcommand{\picdir}{pdffig}
\usepackage[nomarkers,figuresonly]{endfloat}
\usepackage[left=0.7in,right=0.7in,top=0.7in,bottom=0.7in]{geometry}
\newcommand{\numberofpatientsLOOCV}{22 }
\begin{document}
{\color{red}
We thank the reviewers for their insightful remarks and time in review of
this manuscript. A point-by-point response is provided below.
Updates are highlighted in red throughout the manuscript. 
}

Answer to reviewers\\
\\
Reviewer 1:\\
Comments to the Author\\
This is a well-written paper on an evaluation study of
two mathematical models. However, there are several issues with the current version as follow:\\

\begin{enumerate}
\item
The authors stated, in the abstract, that the objective
was to evaluate the trade-off between the
time investment in the algorithm, efficiency of the numerical
implementation, and accuracy required in
predicting the final endpoint of the therapy. However,
the results and discussion were focus on the validation
of the models. It was not obvious what is the trade-off
between the models? 

{\color{red}
The authors agree. The focus of the manuscript was
the accuracy required in
predicting the final endpoint of the therapy. 
Remarks in the abstract on the trade-off between the
time investment in the algorithm, efficiency of the numerical
implementation, and accuracy required in
predicting the final endpoint of the therapy diffuse the focus
of the paper; these remarks have been (1) removed from the abstract and (2)
re-organized as results and  discussion points with model size and
computation time as you suggestion below.  Thank you for this suggestion.
}


\item What is accuracy required in predicting the final endpoint of the therapy?
 
{\color{red}

Regarding the topic of what accuracy is required to consider a model useful
for prediction, it is a multifaceted concept. Firstly, we consider a model
prediction as useful in a particular case if the models ablation region and
the ablation as indicated via MR temperature imaging has a Dice similarity
coefficient (DSC) greater than 0.7. Ultimately the quantitative value of
DSC=.7 is based on prior information and experiences in image processing
literature~\cite{yung2010quantitative,Dice1945measures,zou2004three};  
in which DSC$>$.7 is considered `good agreement'. This point is highlighted
in the discussion section where the DSC is introduced.
 Secondly, the models overall accuracy in
a cohort is measured by the ratio of useful predictions versus inaccurate
predictions (\textit{i.e.}, number of DSC $\geq$ 0.7 versus DSC $<$ 0.7).
This overall accuracy is desired to be as high as possible, but it is beyond the
scope of our present investigation to name a success rate that would
indicate the model would be useful in clinical practice. This manuscript
focuses on reporting the accuracy achieved for our current computational
methods. Future work will consider the accuracy needed to impact treatment 
on a very carefully selected patient population.

}
\item
It is not clear how this method will improve pretreatment planning?

{\color{red}
Long term project goals are to develop a data driven computer model
that can prospectively predict the thermal dose outcome with high
accuracy. The ability to accurately predict the therapy outcome will enable
opportunities to optimize the delivery and maximized tumor kill while
minimizing healthy tissue damage. These efforts present results towards these goals
in the form of a retrospective analysis
with sufficiently large number datasets to evaluate prediction
statistics within a hypothesis testing framework. 
Our LOOCV analysis attempts to mimic our targeted prospective scenario
in that the model is trained on all previous (N-1) datasets and
the trained model prediction accuracy is evaluated on the
independent remaining dataset.
The success or failure of this paradigm is
to be related to several factors. 1) The model used must be able to
describe clinically relevant ablations. 2) There must be a sufficient
quantity of retrospective datasets for the
training to converge. 3) The cohort of retrospective datasets must have sufficient similarity within the
group and to the prediction scenario. \textit{E.g.}, a model trained on datasets from liver ablations is unlikely to
be predictive for brain. However, a training cohort of brain
metastases may or may not have enough homogeneity for a trained model
to be predictive. These points are added to the Discussion section.
}
\item
The introduction is too long. It is not clear how the second part that
describes the hardware relates to the paper.  There is no information
on the model size and the actual computations time.
It is not clear which hardware was used with which particular model. 

{\color{red}
The hardware discussion was intended to
motivate the discussion on the tradeoff between algorithm efficiency and
accuracy on state of the art computing architectures. 
Similar to the previous comment, this diffuses the modeling accuracy focus of the 
manuscript and has been reorganized to a discussion point.
References to quantitatively evaluating computational efficiency as a paper
focus have been removed from the intro.

Runtimes of the models have also been added as discussion points.
The models implemented in the manuscript are 1) a homogenous,
time-dependent FEM model and 2) a homogeneous steady state model.
Intuitively, the FEM model is expected to be more accurate than the steady state model.
There are numerous ways to normalize a comparison between these two
approaches. We chose to report the run-time of a single forward solve. I.E.
given model parameters predict the temperature. This operation is the
lowest common denominator and is the workhorse for the optimization and the
LOOCV analysis. Within this context, the runtime of the optimization and the
LOOCV analysis is the runtime of the single forward solve times the number
of optimization iterations needed or the LOOCV patients considered.
These numbers are intended to qualitatively provide
intuition for the practical run times of these models. 
Steady state model
results are presented from CPU based matlab runs and transient model results are
presented on the GPU cards available to our group. 
Each of these points
is added to the discussion.
Attempts to further optimize and normalize to memory bandwidth or total
flop count are considered outside the scope. 

%but takes considerably longer time
%to run. It is difficult to succinctly express the difference in
%computation times for at least two
%reasons. During optimization for a given patient,
%the FEM model and steady state model may have a different
%number of optimization realizations. Secondly, the FEM model is transient and has multiple time steps within
%one realization whereas the steady state model naturally has a single time step. Therefore, we
%consider the simplest means to summarize computation time is to report the timing of optimization and LOOCV.
%Optimization of the same dataset may involve differing number of realizations for the two models; LOOCV only
%has one forward prediction for each dataset. In optimization, the FEM model takes {\color{green}[Some estimate of time]}; during LOOCV, it took
%{\color{green}[Some estimate of time]}. The steady
%state models LOOCV takes 246.6 seconds or 11.2 seconds on average per dataset. This indicates that both models
%could be used before and during surgery. The Results
%and Discussion have been amended to describe this
%information; the new text is red.
}
\end{enumerate}

Reviewer 2:\\
Comments and suggestions to improve the manuscript:\\
\begin{itemize}
\item
Please expand on any heating induced changes in model parameters. Was blood perfusion held constant for the
entire treatment duration? Blood perfusion is well
known to change during ablative exposures (as the
authors note, this is sometimes exploited to confirm post-ablation heating with contrast-enhanced imaging).  Have the authors explored any techniques for
incorporating temperature/thermal damage dependencies for blood perfusion? Suggest adjusting nominal
perfusion level based on time-temperature history (see, for example, Schutt, Med Phys, 2008).

{\color{red}
Thank you for this suggestion. Model parameters change with temperature was
not considered in this study. 
Adding additional temperature dependet
properties would add one more degree of freedom to 'fit' the data. 
We were concerned with overfitting in this regard.
As the hyperthermia/ablation literature states in concert, tissue properties
are dependent on temperature and thermal damage state and these dependencies
are important to consider in order to accurately predict
outcomes. 
However, this time-dependent phenomenon is mitigated in our investigation via shorter ablation
times relative to other thermal modalities. \textit{E.g.}, laser ablations typically last 1.5 minutes to 2 minutes
whereas the RFA procedures modeled in Schutt, 2008, were 12 and 15 minutes in duration. Furthermore, and
perhaps more importantly, we do not consider the parameters from model optimization (or model
training) to be truly physical. In this investigation, the modeling is driven by model training, not model
complexity/physical fidelity. 
Discussion points for temperature dependent constitutive properties have
been updated.
}
\item
Did the authors investigate the impact of temperature dependent optical properties? During the model
calibration procedure, it appears a single optical attenuation coefficient was determined for all spatial
location/time points. Since optical absorption properties likely change during the course of heating,
one approach may be to perform the model calibration over only the first ~30-60 seconds, to estimate the
nominal optical absorption coefficient. This nominal value could then modified during heating based on
previously published reports of the relationship between optical attenuation and temperature. Suggest
considering approach where

{\color{red}
You are correct that the model calibration arrives at a constant effective
optical attenuation coefficient, eff, for one training dataset. Also, we
certainly agree that patient-specific optimization via test pulse
or initial 30 seconds of ablation is an excellent idea for providing a
calibrated parameter value.
However, we only considered first order
predictions in this manuscript. 
Our investigation is presented such that the present surgical
technique would be mostly unmodified. 
 The surgical planning would be
performed beforehand. 
Using parameter values calibrated by a prior cohort allows the
surgical planning to be performed within a tolerably short period of time.
%During this planning, one prediction would take only
%[such and such] for the FEM model and [such and such] for the steady state model. 
Even in the case that the laser fiber was placed differently than the
designed surgical plan, an updated prediction could be quickly generated. 


Indeed, as you mention,
higher order methods can be considered. 
Conceivably, the additional parameters could compensate for errors.
However, similar to above, the higher order model would also
introduce additional parameters to the optimization.
The resulting analysis on the threshold as which 'over-fitting' would occur
is considered future work.
Furthermore, even in the future case that computation was fast enough for
patient-specific optimization, the surgeon would likely want a pre surgical
`best guess' to inform the surgical plan. This `best guess' is what is
presented in this manuscript.

%The optimization used in calibration requires [such and such] for the FEM model
%and [such and such] for the steady state model. 
%That optimization process could only begin after the low power pulse the
%neurosurgeon uses for position confirmation. If the neurosurgeon is
%satisfied with the localization, ablation would normally follow immediately.
%If a patient-specific optimization was used, the clinician would have to
%wait for the optimization and then check the predicted ablation zone.

%Our rationale for the use of constant parameters is explained in the answer
%to the previous critique you raised.  Finally, the last sentence in this
%critique seems to have been cut short.  Please reiterate it, as necessity dictates.
}
\item
Please state the test performed to confirm the discretization (mesh size, time steps) did not impact the solution.


{\color{red}
Thank you for pointing this out. Discretization schemes were chosen based on
previous studies in canine data~\cite{fuentesetal11a}. 
Quality assurance of the computed result was evaluated using multiple mesh
resolutions to ensure convergence and time dependent manufactured solution
regression tests.  This point has been added to the methods.

%% To ensure convergence, five FEM mesh resolutions were considered for each
%% ROI (min/median/max number of elements = 896/3693/11978, min/median/max
%% number of nodes = 1349/4674/13874). Within the ROI, the mesh size element
%% diameter ranged between 0.5 mm and 2 mm. As a reference, 1 mm is in the
%% order of the pixel size.

%% \paragraph{Goal Oriented Modeling}
%% Goal oriented quantity of interest based error estimates indicate that the
%% mesh
%% resolutions near the resolution of the MR imaging 1mm\textsuperscript{3}
%% accurately represent the solution to the implemented mathematical models,
%% Figure~\ref{QOIEstimates}.  Additionally, the same techniques allow us to
%% gauge the sensitivity
%% of relevant quantities of interest to discretization error; for example, we
%% compared sensitivity of the kill zone and coagulation regions to
%% discretization
%% error in both space and time.
%% The current speed of the GPU solvers is $<$30sec run time for the full
%% 3-5min simulation time
%% and is well within the clinical constraints of the solver.
%% Model reduction techniques are not needed at this time for further
%% efficiency.
%% 
%% \begin{figure}[h]
%% \centering
%% %\begin{tabular}{ccccc}
%% %\scalebox{0.11}{\includegraphics*{\picdir/coarseError.png}} &
%% %\scalebox{0.11}{\includegraphics*{\picdir/qoiCoag.png}} &
%% \scalebox{0.11}{\includegraphics*{\picdir/qoiKillzone.png}}
%% %\scalebox{0.11}{\includegraphics*{\picdir/refinedError.png}} &
%% %\scalebox{0.11}{\includegraphics*{\picdir/refinedErrorRescaled.png}}
%% %\end{tabular}
%% \caption{Goal Oriented Error Estimation.  Goal oriented error estimation
%% techniques have been applied to evaluate discretization error in the laser
%% heating predictions. Two mesh resolutions were construction with differing
%% polynomial order. The average error within the domain of the expected kill
%% zone shows the numerical methods being use is providing adequate accuracy
%% for this application.
%% } \label{QOIEstimates}
%% \end{figure}
%% 

}


\item
 Figure 1, consider blowing up the sketch of applicator (bottom half of fig 1a) so details can be distinguished more easily.

{\color{red} The figure has been modified accordingly.  } 

\item
p6, L24 -- State units of frequency factor and activation energy

{\color{red}
The units have been added.
}

\item
p6, L30 -- can the low-power pulse (used to confirm applicator positioning) be used to estimate optical
attenuation in a patient-specific manner? Did you consider evaluating this dataset and comparing to the
average value obtained from the calibration process for the other datasets?

{\color{red}
We consider it a very good idea to use the low-power pulse as a patient-specific optimization dataset. Also,
your suggestion of comparing the optimized parameter values from the test pulses to the ablative pulses is
very astute. For now, 
our investigation is presented such that the present surgical
technique would be mostly unmodified. Higher order techniques in which
different subsets of the data are considered in the optimization are of
great interest but are considered future work. 
This suggestion has also been added to the discussion as future work.

%Patient-specific optimization is mostly impeded by the fact that it stops
%the surgical workflow. In order to acquire the low-power dataset, the
%neurosurgeon must first place the laser applicator. Then the test pulse
%occurs and is imaged. That dataset is then pushed from the scanner to the
%computational architecture. There, temperature maps must be generated. Only
%then can optimization begin, followed by a forward prediction using the
%optimal optical parameter value. That forward prediction would then be used
%by the neurosurgeon for surgical planning. We expect that the delay in the
%surgery incurred by patient-specific optimization would preclude it being
%useful for some time. 
}
\item
Please modify references so they appear in IJH style.

{\color{red}
The references are updated to match the \textit{IJH} style.
}\\
\item
Figure 3, suggesting use same bin width for both histograms so the results for the two models can be
more readily compared.

{\color{red}
The figure has been amended.
}
\end{itemize}

\begin{thebibliography}{4}

\bibitem{yung2010quantitative}
Joshua~P Yung, Anil Shetty, Andrew Elliott, Jeffrey~S Weinberg, Roger~J
  McNichols, Ashok Gowda, John~D Hazle, and R~Jason Stafford.
\newblock Quantitative comparison of thermal dose models in normal canine
  brain.
\newblock {\em Medical physics}, 37(10):5313--5321, 2010.

\bibitem{Dice1945measures}
L.R. Dice.
\newblock {Measures of the amount of ecologic association between species}.
\newblock {\em Ecology}, 26(3):297--302, 1945.

\bibitem{zou2004three}
K.H. Zou, W.M. Wells~III, R.~Kikinis, and S.K. Warfield.
\newblock Three validation metrics for automated probabilistic image
  segmentation of brain tumours.
\newblock {\em Statistics in medicine}, 23(8):1259--1282, 2004.

\bibitem{fuentesetal11a}
D.~Fuentes, C.~Walker, A.~Elliott, A.~Shetty, J.~Hazle, and R.~J. Stafford.
\newblock {MR temperature imaging validation of a bioheat transfer model for
  LITT}.
\newblock {\em {International Journal of Hyperthermia}}, 27(5):453--464, 2011.
\newblock Cover Page.

\end{thebibliography}
\end{document}
